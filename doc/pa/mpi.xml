<?xml version="1.0" encoding="utf-8"?><chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="WrappingMpiAndLegacyCode"><info><title xml:id="WrappingApplicationLegacycode_main">Wrapping Native MPI Application</title></info>
  <para>The <emphasis role="bold">Message Passing Interface (MPI)</emphasis>
  is a widely adopted communication library for parallel and distributed
  computing. This chapter explains how Proactive can help  you to <emphasis role="bold">deploy</emphasis>,  <emphasis role="bold">wrap</emphasis>  and
  <emphasis role="bold">couple</emphasis>  MPI applications for distributed
  environments</para>

<note><para><emphasis role="bold">Please note that, for the moment, the wrapping of native MPI applications is only available for *nix Operating Systems</emphasis></para></note>

  <para>Three different MPI wrapping approaches are possible:</para>
  <itemizedlist>

  	<listitem>
          <para><emphasis role="bold"> Simple deployment of unmodified MPI applications (<xref linkend="Simple_Deploying_legacy"/>) </emphasis></para>
          <para> You will follow this approach if you want to take profit of ProActive GCM Deployment 
           to handle deployment issues (access, allocation and remote execution) 
           to deploy your existing MPI applications.
          </para>
   </listitem>

   <listitem>
          <para><emphasis role="bold"> MPI code coupling (<xref linkend="MPI_code_coupling"/>) </emphasis></para>
          <para> This wrapping approach was designed to couple independent standalone MPI applications to make them interoperate,  using a ProActive layer for inter-application communication.
          </para>
   </listitem>

   <listitem>
          <para><emphasis role="bold"> MPI code wrapping  (<xref linkend="MPI_code_wrapping"/>) </emphasis></para>
          <para> This wrapping approach consist on supporting "MPI to/from Java" communications which permit users to exchange data between MPI and Java applications.
          </para>
   </listitem>
  </itemizedlist>


<para> The MPI Native Applications Wrapping is organized along three different packages: 
</para>

<itemizedlist>
 	<listitem>
          <para> <literal> org.objectweb.proactive.extensions.nativeinterface </literal>: contains the implementation of code wrapping ( Java-C communication, data conversion and management)
          </para>
    </listitem>

 	<listitem>
          <para> <literal>org.objectweb.proactive.extensions.nativeinterfacempi </literal>: contains the implementation of code coupling of MPI applications (including data conversion and MPI deployment management)
          </para>
    </listitem>

 	<listitem>
          <para> <literal> org.objectweb.proactive.extensions.nativecode </literal>:  contains a simple bootstrap class to launch wrapped distributed/parallel applications
          </para>
    </listitem>
</itemizedlist>


<section xml:id="Simple_Deploying_legacy"><info><title xml:id="Simple_Deploying_legacy_5">Simple Deployment of unmodified MPI applications</title></info>

	<para> As the name says, the simple deployment of MPI application is intended to deploy standalone MPI applications. It does not require the usage of any specific API.
	  It is just a matter of defining the application deployment in terms of GCMD and GCMA descriptors and loading/deploying these descriptors. (in  <xref linkend="GCMDeployment"/> , you can retrieve more information about GCM Deployment).
	   The advantage of deploying an MPI application with GCM deployment is that you can benefit from de support to various network protocols and grid/cloud middlewares to transparently deploy 
	   your MPI applications without configuring of the environment or usaging of specific tools.
	</para>
	
	<para> The following descriptors are part of an application example, which can be deployed with the script <literal> HelloExecutableMPI.sh </literal> available in <literal> ProActive/examples/mpi/standalone </literal>. 
	</para>
	
	<para> Initially, the GCMA application must be defined. The following snippet of code shows de definition of an MPI application, in this case a simple application that implements a parallel “hello world” in MPI:
	</para>
	
	<programlisting language="xml"><textobject><textdata  fileref="automatic_snippets/GCMA_MPI_Descriptor.snip"/></textobject></programlisting>
	
	<para>The definition of node provider follows the philosophy of a Java application deployment. But, instead of using an SSH (or any other) group, the group is defined as an MPI group.  The following snippet of descriptor shows a simple GCMD which defines the deployment of the application over a set of distributed resources :
	</para>		
	
	<programlisting language="xml"><textobject><textdata  fileref="automatic_snippets/GCMD_MPI_Descriptor.snip"/></textobject></programlisting>	

<para> Please not that, on the definition of the MPI group, some different tags can be used. The following tags are mandatory: </para>

 
<itemizedlist>
	<listitem>
          <para><emphasis role="bold"> hostlist </emphasis>: list of resources where the MPI application will be deployed;
          </para>
   </listitem>

	<listitem>
          <para><emphasis role="bold"> distributionPath </emphasis>: installation path of MPI. In the example shown above, MPI is supposed to be installed system-wide. So, the <literal>mpirun</literal> command can be found in <literal>/usr/bin</literal>;
          </para>
   </listitem>

	<listitem>
          <para><emphasis role="bold"> commandPath </emphasis>:  the deployment of MPI application by ProActive is actually done by scripts which are responsible for the configuration of the environment, before the execution. This is also the place where the runtime handles the different MPI distribution characteristics (e.g. LAM/MPI requires the creation of a virtual lam environment).
          </para>

		  <para> The ProActive middleware offer support to four different MPI distributions: </para>

		  <itemizedlist>	
			<listitem>
		          <para>LAM/MPI: use the <literal>ProActive/scripts/gcmdeployment/executable_mpi_lam.sh </literal> commandPath;
		          </para>
		   </listitem>

			<listitem>
		          <para>MPICH or OpenMPI: use the <literal>ProActive/scripts/gcmdeployment/executable_mpi_mpich.sh </literal> commandPath;
		          </para>
		   </listitem>

			<listitem>
		          <para>GridMPI: use the <literal>ProActive/scripts/gcmdeployment/ProActive/scripts/gcmdeployment/executable_gridmpi.sh</literal> commandPath.
		          </para>
		   </listitem>
		 </itemizedlist>

		<para>But you can use any other distribution by defining a similar scripts and using them on GCMD </para>
   </listitem>

</itemizedlist>	

<para>The following tags are optional on the definition of <emphasis role="bold">MPIGroups </emphasis>:</para>

<itemizedlist>
	<listitem>
          <para><emphasis role="bold"> commandOptions </emphasis>: list of MPI-related option to be appended on the <literal>mpirun </literal> command at deployment time;
          </para>
   </listitem>

	<listitem>
          <para><emphasis role="bold"> machineFile </emphasis>: file containing a list of machines. If this file is not provided, the runtime create automatically the <literal>machineFile</literal>, including the hostnames defined in the <literal>hostlist</literal> tag. If the file is generated at runtime, the file is deleted prior to the end of the execution;
          </para>
   </listitem>

	<listitem>
          <para><emphasis role="bold"> execDir </emphasis>:  directory where the <literal>mpirun</literal> command will be execute. This allows the usage of relative path on the definition of the MPI binary on the GCMA descriptor.
          </para>
   </listitem>

</itemizedlist>


<para>Once you have defined the 2 descriptors (GCMA and GCMD), the deployment of the application is pretty straightforward:</para>

<programlisting language="java"><textobject><textdata  fileref="automatic_snippets/Hello_MPI_example.snip"/></textobject></programlisting>	

</section>

<section xml:id="MPI_code_coupling"><info><title xml:id="MPI_code_coupling_5"> MPI Code Coupling</title></info>
<para>
	 This wrapping approach was designed to couple independent standalone MPI applications to make them interoperate,  using a  a dedicated MPI-like communication API and seamless using a ProActive communication inter-application layer.  Besides the ease of deployment, the MPI code coupling can be used to perform transparent inter-cluster communication, even if nodes does not present a direct link among them.
</para>
<para> There are two basic usages for the ProActive/MPI coupling approach: </para>
	
<itemizedlist>	
	<listitem>
          <para><emphasis role="bold"> Single MPI application </emphasis>: the idea of code coupling in the context of a single application is to make possible the execution of a single MPI application across a multi-domain environment, which could composed by clusters, grids and/or cloud resources.  This usage is specially intended for embarrassingly distributed/parallel applications or applications parallelized upon a domain-decomposition approach. For instance, you might have a large data mesh and intend to distribute more this mesh to avoid memory swapping and cash misses.
          </para>
   </listitem>

	<listitem>
          <para><emphasis role="bold"> Multiple MPI applications </emphasis>: this is a more loosely coupled approach which makes capable to loose-coupling multiple MPI applications which can be deployed in different clusters, grids and/or cloud resource. This usage is specially intended for distributed and parallel applications parallelized upon a functional-decomposition approach. For instance, in a typical weather modeling, you might have multiple independent numerical kernels which implement different models (hydrology, pressure and temperature, for instance), processing independent sets of data and explicit data dependencies among these kernels.
          </para>
   </listitem>
</itemizedlist>

<para> In both cases, the MPI-coupling scenario looks like the following picture:</para>
 <para><figure xml:id="MPI_Coupling2"><info><title>MPI-coupling scenario</title></info>
      <mediaobject>
        <imageobject>
          <imagedata scalefit="1" width="100%" contentdepth="100%" fileref="mpi_files/mpi-coupling.png" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>
</para>

<para> On this picture, we can identify two independent MPI applications (Application 0 and Application 1) running in two sets of resources. Internally, each of the applications communicate through standard MPI and the two applications can communicate through the ProActive/MPI communication layer, due to an API defined on <xref linkend="MPI_code_coupling_API"/> . 
</para>
<para> Each of the process receive a hierarchical identification <literal>X:Y</literal>, being <literal>X</literal> the application ID and <literal>Y</literal> the MPI rank. By using the API and the hierarchical ranks, the different applications can address process of different applications to communicate.
</para>

<section xml:id="MPI_code_coupling_API"><info><title xml:id="MPI_code_coupling_API_5"> MPI Code Coupling API</title></info>
<para> The MPI Code Coupling API is a simplified MPI-like set of primitives, exposed through C/C++ and Fortran bindings,  intended to enable the explicit communicating among the different MPI applications. Since ProActive plays just a middleware role in the case of code coupling, there is no Java API associated to code coupling. <xref linkend="MPI_code_wrapping"/> presents an API that allows Native-Java communication.
</para>

       
<!-- ///// ProActive API Definition \\\\ -->

<!-- ProActiveMPI_Init  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Init</emphasis>
    Init ProActiveMPI environment, bind the native process to the Java wrapping process.

<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Init(int rank);

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">rank</emphasis>  the MPI rank of the process.
</programlisting>

<!-- ProActiveMPI_Finalise  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Finalize</emphasis>
   Finalize ProActiveMPI infrastructure.
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Finalize();

</programlisting>

<!-- ProActiveMPI_Job  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Job</emphasis>
   Finalize ProActiveMPI infrastructure.
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Job(int * job, int * nb_process);
	
	<emphasis role="bold">Output Parameters</emphasis>
	<emphasis role="bold">job</emphasis> the number of coupled independent MPI applications  
	<emphasis role="bold">nb_process</emphasis> the global numer of processes involved in the coupled application  
</programlisting>

<!-- ProActiveMPI_Send  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Send</emphasis>
	Sends data to a remote process, possibly running on a different MPI application.
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Send(void* buf, int count, MPI_Datatype datatype, int dest, int tag, int idjob ); 

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">buf</emphasis>      the buffer to be sent 
    <emphasis role="bold">count</emphasis>    number of elements in send buffer (nonnegative integer) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">dest</emphasis>     rank of destination (integer) 
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">jobID</emphasis>    remote job (integer)
</programlisting>

<!-- ProActiveMPI_ISend  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_ISend</emphasis>
	Sends asynchronously data to a remote process, possibly running on a different MPI application.
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_ISend(void* buf, int count, MPI_Datatype datatype, int dest, int tag, int idjob, ProActiveMPI_Request *r ); 

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">buf</emphasis>      the buffer to be sent 
    <emphasis role="bold">count</emphasis>    number of elements in send buffer (nonnegative integer) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">dest</emphasis>     rank of destination (integer) 
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">jobID</emphasis>    remote job (integer)
    <emphasis role="bold">request</emphasis>  communication request (handle)  
</programlisting>

<!-- ProActiveMPI_Recv  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Recv</emphasis>
    Performs a blocking receive waiting from data from ProActive java side (eventually dispatched from a distant MPI process)

<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Recv(void *buf, int count, MPI_Datatype datatype, int src, int tag, int jobID);

<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">buf</emphasis>   initial address of receive buffer  

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">count</emphasis>    number of elements in send buffer (nonnegative integer) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">src</emphasis>      rank of source (integer) 
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">jobID</emphasis>    remote job (integer)
</programlisting>

        
<!-- ProActiveMPI_IRecv  -->

<programlisting language="java">
<emphasis role="bold">ProActiveMPI_IRecv</emphasis>
    Performs a non blocking receive from MPI side to receive data from ProActive java side (eventually dispatched from a distant MPI process)

<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_IRecv(void* buf, int count, MPI_Datatype datatype, int src, int tag, int idjob, ProActiveMPI_Request * request);

<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">request</emphasis>   communication request (handle)  

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">buf</emphasis>      initial address of receive buffer  
    <emphasis role="bold">count</emphasis>    number of elements in send buffer (nonnegative integer) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">src</emphasis>      rank of source (integer) 
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">jobID</emphasis>    remote job (integer)
</programlisting>


<!--ProActiveMPI_Bcast-->
<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Bcast</emphasis>
    Performs a broadcast to an group of processe

<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Bcast(void * sendbuf, int count, MPI_Datatype datatype, int tag, int nb_send, int * pa_rank_array);

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">sendbuf</emphasis>    initial address of sender buffer  
    <emphasis role="bold">count</emphasis>    number of elements in send buffer (nonnegative integer) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">nb_send</emphasis> the ID of the sender  
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">jobID</emphasis>    remote job (integer)
    <emphasis role="bold">pa_rank_array</emphasis>  array of destinations
</programlisting>

<!--ProActiveMPI_Scatter-->
<programlisting language="java">
<emphasis role="bold">ProActiveMPI_Scatter</emphasis>
 Scatter a set of buffers to a set of destinations

<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
	int ProActiveMPI_Scatter(int nb_send, void ** buffers, int * count_array, MPI_Datatype datatype, int tag, int * pa_rank_array);

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">buffers</emphasis>   array of pointers to buffers  
    <emphasis role="bold">count</emphasis>    array of integer containing number of elements in each send buffer (nonnegative integers) 
    <emphasis role="bold">datatype</emphasis> datatype of each recv buffer element  
    <emphasis role="bold">tag</emphasis>      message tag (integer) 
    <emphasis role="bold">pa_rank_array</emphasis>  array of destinations
</programlisting>

        <!-- ProActiveTest  -->

        <programlisting language="java">
<emphasis role="bold">ProActiveMPI_Test</emphasis>
    Tests for the completion of receive from a ProActive java class
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
    int ProActiveMPI_Test(ProActiveMPI_Request *request, int *flag);

<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">flag</emphasis>     true if operation completed (logical)  

<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">request</emphasis>  communication request (handle)  
</programlisting>

        <!-- ProActiveWait  -->

        <programlisting language="java">
<emphasis role="bold">ProActiveMPI_Wait</emphasis>
    Waits for an MPI receive from a ProActive java class to complete
<emphasis role="bold">Synopsis</emphasis>
    #include "ProActiveMPI.h"
    int ProActiveMPI_Wait(ProActiveMPI_Request *request);

<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">request</emphasis>  communication request (handle)  
</programlisting>

</section>



<section xml:id="MPI_code_coupling_Ex"><info><title xml:id="MPI_code_coupling_ex_5"> MPI Code Coupling Deployment and Example</title></info>

<para> This section presents an example of code coupling available in <literal> ProActive/src/Examples/org/objectweb/proactive/examples/mpi/proactive_mpi/</literal>. The following code is the MPI implementation of a small benchmark of the point-to-point synchronous and asynchronous communication between processes in two different MPI applications: 
</para>

	<programlisting language="c"><textobject><textdata fileref="automatic_snippets/MPI_Coupling_Example.snip"/></textobject></programlisting>
	
<para> In general, the structure of a ProActive-coupled MPI application application looks like a standard MPI application: it starts by an initial handshake to start the ProActive/MPI application (ProActiveMPI_Init) and obtain identifiers (MPI_Comm_rank and ProActiveMPI_Job). Then, the application core, which may contain the numerical kernels which involve communication and thus the usage of MPI and ProActiveMPI applications. And, at the end, the finalization of the application (ProActiveMPI_Finalize).
</para>

<note><para><emphasis role="bold">Please note that ProActive/MPI applications have a communication semantic similar to MPI applications. This means that, despite the OO implementation of the communication layer, the communications are, in general, two-side operations (send/receive). The transport implementation in the context of the ProActive layer is completely asynchronous, but there is no relation between the asynchronous calls with futures and the Native-side of applications. For this reason, you should, as in MPI, prefer asychronous communications over synchronous ones to improve applications performance.</emphasis></para></note>

<para> Since this is a ProActive example, it can be simply compiled within the Ant target <literal> compile.extensions </literal>. But, you may need to compile/link your applications with ProActiveMPI native libraries in order to deploy your own applications. In order to do so, you will require the following steps: 
</para>
	
	
<orderedlist>			
	<listitem>
          <para> First you must be under an *nix system and have MPI installed;</para>
   </listitem>
		
	<listitem>
          <para> Compile the <literal> compile.extensions </literal> Ant target to create native libraries; </para>
   </listitem>
	
	<listitem>
          <para> Include the folder <literal>ProActive/dist/lib/native </literal> (<literal>-L</literal>ProActive/dist/lib/native); </para>
   </listitem>
	
	<listitem>
         <para> Include the folders <literal> ProActive/classes/Extensions/org/objectweb/proactive/extensions/nativeinterface/ </literal> and <literal> ProActive/classes/Extensions/org/objectweb/proactive/extensions/nativeinterfacempi/control/config/src/ </literal> to the compilation  (<literal>gcc / mpicc -I</literal>);</para>
   </listitem>

	<listitem>
          <para> Include the libraries <literal> rt</literal>,  <literal> lProActiveNativeInterfaceIPC </literal> and <literal>ProActiveMPIComm </literal>  ( <literal> -lProActiveNativeInterfaceIPC -lProActiveMPIComm </literal>).</para>
   </listitem>

</orderedlist>
	
<para> So, your compilation command will look a bit like: </para>

<screen> mpicc -O3 -lrt -I${ProActive}/classes/Extensions/org/objectweb/proactive/extensions/nativeinterfacempi/control/config/src/
-I${ProActive}classes/Extensions/org/objectweb/proactive/extensions/nativeinterface/ -L${ProActive}/dist/lib/native -lProActiveNativeInterfaceIPC -lProActiveMPIComm your_application.c -o your_application 
</screen>
	
	<para>Once you have your application compiled, you will have to write some deployment descriptors, for: java communication layer and each of the independent MPI applications to be deployed. </para>
	
	<para> The Java descriptor looks exactly like the descriptors presented in <xref linkend="GCMDeployment"/>. The only extra requirement is the definition of  a <literal> -Djava.library.path </literal> option which is the place where the Java runtime will load the native library that allons Java-C communication. This options must point to <literal> ProActive/dist/lib/native</literal>
	</para>
	
	<para> ProActiveMPI GCMA: </para>
	
	<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_PA_GCMA.snip"/></textobject></programlisting>
	
	<para> In the GCMA definition, you must define as many resource providers as independent MPI applications</para>
	<note><para><emphasis role="bold">Make attention that the resources used for the runtime deployment must be the same used for MPI execution</emphasis></para></note>
	
	<para> ProActiveMPI GCMD: </para>
	 
	<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_PA_GCMD.snip"/></textobject></programlisting>
		
	<para> Note that we have defined the deployment of one runtime by JVM, because the ProActiveMPI runtime are singleton and then,  it is important to avoid runtime co-allocation</para>
	
	<para> The MPI deployment descriptors are exactly like the ones presented in <xref linkend="Simple_Deploying_legacy"/> </para>
	
	<para> MPI GCMA: </para>
	
	<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_MPI_GCMA.snip"/></textobject></programlisting>
		
	<para> MPI GCMD: </para>
	
	<programlisting language="xml"><textobject><textdata fileref="automatic_snippets/MPI_coupling_MPI_GCMD.snip"/></textobject></programlisting> 
	
	<para> After you have defined these descriptors you can make use of the <literal> org.objectweb.proactive.extensions.nativecode.NativeStarter </literal> class. This class receive, as parameters, the Java ProActive GCMA, and the MPI GCMAs: </para>
	
	<screen> org.objectweb.proactive.extensions.nativecode.NativeStarter gcma.pa.xml gcma.mpi1.xml gcma.mpi2.xml ... </screen>
</section>

<section xml:id="DiscoGrid"><info><title xml:id="DiscoGrid_5"> The DiscoGrid Project</title></info>

<para>The <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www-sop.inria.fr/nachos/team_members/Stephane.Lanteri/DiscoGrid/">DiscoGrid</link> project aims at studying and promoting a new paradigm for programming non-embarrassingly parallel scientific computing applications on distributed, heterogeneous, computing platforms. The target applications require the numerical resolution of systems of partial differential equations (PDEs) modeling electromagnetic wave propagation and fluid flow problems.
</para>
<para>In the context of the DiscoGrid project, the ProActive team developed a GCM component-based runtime capable of coupling and deploying MPI applications over heterogeneous multi-domain infrastructures, composed by clusters, grids and clouds with the seamless treatment of complex network configurations (including firewalls and NAT).
</para>

<para> If you are interested in more advanced usage of the ProActive/MPI code coupling approach, you will probably be interested in the DiscoGrid project.  Please access the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://gforge.inria.fr/projects/dg-proactive/"> the DiscoGrid forge</link> for documentation and download of the DiscoGrid distribution.</para>

</section>


</section> <!--closes MPI Code Coupling-->

<section xml:id="MPI_code_wrapping"><info><title xml:id="MPI_code_wrapping_5"> MPI Code Wrapping</title></info>

<para>This wrapping approach was designed to couple native applications and Java codes make them interoperate by the usage of a simplified message passing-based API. This approach allows the development of native applications or numerical kernels to be bind to  a native implementation. </para>

<para>
Two usage scenarios motivate this wrapping approach:
</para>

<itemizedlist>	
<listitem>
      <para><emphasis role="bold"> Usage of existing numerical kernels or applications </emphasis>: supposing you want to have part of your Java application to be handles by an existing native code to improve performance and/or simply reuse these native kernels instead of translating them to Java. This is particularly useful for highly optimized native applications which can not be easily ported to Java.
      </para>
</listitem>

<listitem>
      <para><emphasis role="bold"> Usage of resources not available in Java (libraries, CUDA, etc.) </emphasis>: unfortunately, a number of libraries are just available in other programming languages (such as C/C++ and Fortran). This is the case of numerous numerical libraries and libraries to handle hardware devices (for instance, CUDA computing engine).
      </para>
</listitem>
</itemizedlist>

<para>  In both cases, all you need is to perform a deployment of the native application (<xref linkend="Executable_64"/>), the deployment of the Java application (<xref linkend="GCMDeployment"/>) and use an specific API to make them communicate. The compilation of the native application and deployment follows exactly the same approach presented in <xref linkend="MPI_code_coupling"/>. The main difference relies on the usage of your own Java application instead of the <literal> NativeStarter</literal>.  </para>

<section xml:id="Code_wrapping_API"><info><title xml:id="Code_wrapping_API_5"> Code Wrapping API</title></info>
<para> The MPI Code Wrapping API is a simplified message passing based set of primitives, exposed on the native side through C/C++ bindings (<xref linkend="MPI_code_coupling_API"/>), and with a couple of API classes on the Java side.
</para>


<section xml:id="Native_code_wrapping_API"><info><title xml:id="Native_code_wrapping_API_5"> Native Code Wrapping API</title></info>

<para> These are the main primitives available to init/finalize the binding of native and Java applications and communicate native-java processes: </para>

<!--init-->
<programlisting language="java">
<emphasis role="bold">init</emphasis>
 Start the handshake that binds the native and Java application

<emphasis role="bold">Synopsis</emphasis>
    #include "native_layer.h"

	int init(int creation_flag);

	<emphasis role="bold">Input Parameters</emphasis>
    <emphasis role="bold">creation_flag</emphasis>  Singe the implementation of the IPC communication is shared by Java/Native, this flag identifies the origin of the function call (if native side,  creation_flag = 0 else creation_flag = 1)
</programlisting>


<!--terminate-->
<programlisting language="java">
<emphasis role="bold">terminate</emphasis>
 Terminate the wrappingm releasing memory and cleaning up running environment
<emphasis role="bold">Synopsis</emphasis>
    #include "native_layer.h"

	int terminate();
</programlisting>

<!--recv_message-->
<programlisting language="java">
<emphasis role="bold">init</emphasis>
 Receive message from Java wrapper
<emphasis role="bold">Synopsis</emphasis>
    #include "native_layer.h"

	int recv_message(int * length, void ** data_ptr);

	<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">lenght</emphasis> size in bytes of the buffer that will be received
    <emphasis role="bold">data_ptr</emphasis> pointer to the byte buffer
</programlisting>


<!--recv_message_async-->
<programlisting language="java">
<emphasis role="bold">init</emphasis>
 Receive message asynchronously from Java wrapper
<emphasis role="bold">Synopsis</emphasis>
    #include "native_layer.h"

	int recv_message_async(int * length, void ** data_ptr);

	<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">lenght</emphasis> size in bytes of the buffer that will be received
    <emphasis role="bold">data_ptr</emphasis> pointer to the byte buffer
</programlisting>

<!--send_message-->
<programlisting language="java">
<emphasis role="bold">init</emphasis>
 Send message to Java wrapper
<emphasis role="bold">Synopsis</emphasis>
    #include "native_layer.h"

	int send_message(int length, void *data_ptr);

	<emphasis role="bold">Output Parameters</emphasis>
    <emphasis role="bold">lenght</emphasis> size in bytes of the buffer that will be received
    <emphasis role="bold">data_ptr</emphasis> pointer to data structure that indicated the message to be sent (message includes data type, signature and buffer pointer)
</programlisting>


</section>  <!--Closes Native API-->


<section xml:id="Java_code_wrapping_API"><info><title xml:id="Java_code_wrapping_API_5"> Java Code Wrapping API</title></info>
<para>The Java API is mainly offered thorugh 2 classes: </para>


 <itemizedlist>

    <listitem>
          <para> Communication and Code Coupling, on the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="../api_complete/org/objectweb/proactive/extensions/nativeinterface/coupling/NativeInterface.html"> <literal>org.objectweb.proactive.extensions.nativeinterface.coupling.NativeInterface</literal>  </link> class. </para>
   </listitem>

    <listitem>
          <para> Data conversion, on the <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="../api_complete/org/objectweb/proactive/extensions/nativeinterface/utils/ProActiveNativeUtil.html"> <literal>org.objectweb.proactive.extensions.nativeinterface.utils.ProActiveNativeUtil</literal>  </link> class. </para>
   </listitem>

  </itemizedlist>



<para> If you want a practical use of these APIs, please check <xref linkend="MPI_code_coupling"/> and the Java and C implementations
 of the MPI Code coupling in the ProActive package <literal>org.objectweb.proactive.extensions.nativeinterfacempi </literal> </para>

</section> <!--Closes Java API-->

</section> <!--Closes API-->


</section>




</chapter>
